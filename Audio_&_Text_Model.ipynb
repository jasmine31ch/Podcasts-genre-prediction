{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "toc_visible": true,
      "authorship_tag": "ABX9TyO7eMExiTdcfoP/GpjmdEV9"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oe6trmOBB287",
        "outputId": "8b24362a-afb5-4869-de6a-670b31ac9d68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "datadir = \"/content/drive/MyDrive/CS441/FP2/data/snippets\"\n",
        "save_dir = \"/content/drive/My Drive/CS441/FP2\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchcodec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTvdkd1M1vKI",
        "outputId": "0e6d47c2-2c2a-402f-a242-bd8a53860138"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchcodec in /usr/local/lib/python3.12/dist-packages (0.8.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchaudio\n",
        "from transformers import (\n",
        "    DistilBertTokenizerFast,\n",
        "    DistilBertModel,\n",
        "    Wav2Vec2FeatureExtractor,\n",
        "    Wav2Vec2Model\n",
        ")\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "kOBnq3pexegn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PodcastDataset(Dataset):\n",
        "    def __init__(self, csv_path, audio_root, tokenizer, feature_extractor, max_audio_len=5.0):\n",
        "        self.data = pd.read_csv(csv_path)\n",
        "        self.audio_root = audio_root\n",
        "        self.tokenizer = tokenizer\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.max_audio_len = max_audio_len\n",
        "\n",
        "        genres = sorted(self.data[\"genre\"].unique())\n",
        "        self.label_map = {g: i for i, g in enumerate(genres)}\n",
        "        print(f\"ðŸ“Š Loaded {len(self.data)} samples, {len(genres)} classes: {genres}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "\n",
        "        # ----- Audio -----\n",
        "        audio_path = os.path.join(self.audio_root, \"snippets\", row[\"genre\"], row[\"path\"])\n",
        "        if not os.path.exists(audio_path):\n",
        "            raise FileNotFoundError(f\"Audio not found: {audio_path}\")\n",
        "\n",
        "        wav, sr = torchaudio.load(audio_path)\n",
        "\n",
        "        # Switched to mono\n",
        "        if wav.size(0) > 1:\n",
        "            wav = wav.mean(dim=0, keepdim=True)\n",
        "\n",
        "        # resample to 16kHz\n",
        "        if sr != 16000:\n",
        "            resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n",
        "            wav = resampler(wav)\n",
        "            sr = 16000\n",
        "\n",
        "        # Crop or padding to a fixed length\n",
        "        target_len = int(self.max_audio_len * sr)\n",
        "        if wav.size(1) > target_len:\n",
        "            wav = wav[:, :target_len]\n",
        "        elif wav.size(1) < target_len:\n",
        "            pad_len = target_len - wav.size(1)\n",
        "            wav = torch.nn.functional.pad(wav, (0, pad_len))\n",
        "\n",
        "        # Normalized audio\n",
        "        wav = wav / (wav.abs().max() + 1e-8)\n",
        "\n",
        "        audio_features = self.feature_extractor(\n",
        "            wav.squeeze().numpy(), sampling_rate=sr, return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # ----- Text -----\n",
        "        text = row[\"transcript\"]\n",
        "        max_len = 256\n",
        "\n",
        "        if pd.isna(text) or text.strip() == \"\":\n",
        "            use_text = False\n",
        "            input_ids = torch.zeros(max_len, dtype=torch.long)\n",
        "            attention_mask = torch.zeros(max_len, dtype=torch.long)\n",
        "        else:\n",
        "            use_text = True\n",
        "            encoded_text = self.tokenizer(\n",
        "                text,\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "                max_length=max_len,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "            input_ids = encoded_text[\"input_ids\"].squeeze(0)\n",
        "            attention_mask = encoded_text[\"attention_mask\"].squeeze(0)\n",
        "\n",
        "        label = self.label_map[row[\"genre\"]]\n",
        "\n",
        "        return {\n",
        "            \"audio_values\": audio_features[\"input_values\"].squeeze(0),\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": torch.tensor(label),\n",
        "            \"use_text\": torch.tensor(use_text, dtype=torch.bool)\n",
        "        }"
      ],
      "metadata": {
        "id": "OrC2gmWVxhSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImprovedAudioTextClassifier(nn.Module):\n",
        "    def __init__(self, num_classes, freeze_pretrained=False):  # unfreeze\n",
        "        super().__init__()\n",
        "\n",
        "        # Audio encoder\n",
        "        self.audio_encoder = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "        # Text encoder\n",
        "        self.text_encoder = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "        if freeze_pretrained:\n",
        "            for param in self.audio_encoder.parameters():\n",
        "                param.requires_grad = False\n",
        "            for param in self.text_encoder.parameters():\n",
        "                param.requires_grad = False\n",
        "        else:\n",
        "            # Only fine-tune the last few layers\n",
        "            # Freeze the first 4 layers of Wav2Vec2\n",
        "            for i, layer in enumerate(self.audio_encoder.encoder.layers):\n",
        "                if i < 4:\n",
        "                    for param in layer.parameters():\n",
        "                        param.requires_grad = False\n",
        "\n",
        "            # Freeze the first 2 layers of DistilBERT\n",
        "            for i, layer in enumerate(self.text_encoder.transformer.layer):\n",
        "                if i < 2:\n",
        "                    for param in layer.parameters():\n",
        "                        param.requires_grad = False\n",
        "\n",
        "        # Improved Projection Layer (with BatchNorm and Dropout)\n",
        "        self.audio_proj = nn.Sequential(\n",
        "            nn.Linear(768, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "\n",
        "        self.text_proj = nn.Sequential(\n",
        "            nn.Linear(768, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "\n",
        "        # Attention Fusion Layer\n",
        "        self.fusion_attention = nn.Sequential(\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(512, 2),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "        # Improved classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_audio, input_ids=None, attention_mask=None, use_text=None):\n",
        "        # ===== audio features =====\n",
        "        audio_out = self.audio_encoder(input_audio).last_hidden_state\n",
        "        audio_feat = audio_out.mean(dim=1)  # [batch_size, 768]\n",
        "        audio_feat = self.audio_proj(audio_feat)  # [batch_size, 512]\n",
        "\n",
        "        # ===== text features =====\n",
        "        batch_size = audio_feat.size(0)\n",
        "        text_feat = torch.zeros(batch_size, 512, device=audio_feat.device, dtype=audio_feat.dtype)\n",
        "\n",
        "        if use_text is not None:\n",
        "            text_mask = use_text.bool()\n",
        "\n",
        "            if text_mask.any():\n",
        "                valid_input_ids = input_ids[text_mask]\n",
        "                valid_attention_mask = attention_mask[text_mask]\n",
        "\n",
        "                text_out = self.text_encoder(\n",
        "                    input_ids=valid_input_ids,\n",
        "                    attention_mask=valid_attention_mask\n",
        "                ).last_hidden_state\n",
        "\n",
        "                text_feat_valid = text_out.mean(dim=1)\n",
        "                text_feat_valid = self.text_proj(text_feat_valid)\n",
        "                text_feat[text_mask] = text_feat_valid\n",
        "\n",
        "        # ===== Attention Fusion =====\n",
        "        fused = torch.cat([audio_feat, text_feat], dim=1)  # [batch_size, 1024]\n",
        "\n",
        "        # Calculate attention weights\n",
        "        attn_weights = self.fusion_attention(fused)  # [batch_size, 2]\n",
        "\n",
        "        # Weighted fusion\n",
        "        weighted_audio = audio_feat * attn_weights[:, 0:1]\n",
        "        weighted_text = text_feat * attn_weights[:, 1:2]\n",
        "        final_feat = weighted_audio + weighted_text  # [batch_size, 512]\n",
        "\n",
        "        logits = self.classifier(final_feat)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "iCcc10Ekxk9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train / Eval\n",
        "# =====================================\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            logits = model(\n",
        "                input_audio=batch[\"audio_values\"].to(device),\n",
        "                input_ids=batch[\"input_ids\"].to(device),\n",
        "                attention_mask=batch[\"attention_mask\"].to(device),\n",
        "                use_text=batch[\"use_text\"].to(device)\n",
        "            )\n",
        "            preds = logits.argmax(dim=1)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy, all_preds, all_labels"
      ],
      "metadata": {
        "id": "IkGbAO4px0Z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(train_csv, val_csv, audio_root, batch_size=8, lr=2e-5, epochs=10, warmup_epochs=2):\n",
        "    tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
        "    feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "    train_ds = PodcastDataset(train_csv, audio_root, tokenizer, feature_extractor)\n",
        "    val_ds = PodcastDataset(val_csv, audio_root, tokenizer, feature_extractor)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    num_classes = len(train_ds.label_map)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # ImprovedAudioTextClassifier\n",
        "    model = ImprovedAudioTextClassifier(num_classes=num_classes, freeze_pretrained=False).to(device)\n",
        "\n",
        "    # different learning rate\n",
        "    pretrained_params = []\n",
        "    new_params = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'audio_encoder' in name or 'text_encoder' in name:\n",
        "            pretrained_params.append(param)\n",
        "        else:\n",
        "            new_params.append(param)\n",
        "\n",
        "    optimizer = AdamW([\n",
        "        {'params': pretrained_params, 'lr': lr * 0.1},  # Pre-training layer with smaller learning rate\n",
        "        {'params': new_params, 'lr': lr}\n",
        "    ])\n",
        "\n",
        "    # Add a learning rate scheduler\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    best_acc = 0\n",
        "    patience = 5\n",
        "    no_improve = 0\n",
        "\n",
        "    print(f\"\\nðŸš€ Starting training...\")\n",
        "    print(f\"Device: {device}\")\n",
        "    print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "    print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\\n\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "        for batch in pbar:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            logits = model(\n",
        "                input_audio=batch[\"audio_values\"].to(device),\n",
        "                input_ids=batch[\"input_ids\"].to(device),\n",
        "                attention_mask=batch[\"attention_mask\"].to(device),\n",
        "                use_text=batch[\"use_text\"].to(device)\n",
        "            )\n",
        "            loss = criterion(logits, batch[\"labels\"].to(device))\n",
        "\n",
        "            if torch.isnan(loss) or torch.isinf(loss):\n",
        "                print(f\"\\nâš ï¸ Warning: Invalid loss detected, skipping batch\")\n",
        "                continue\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        val_acc, _, _ = evaluate(model, val_loader, device)\n",
        "\n",
        "        print(f\">>> Epoch {epoch+1} | Train Loss: {avg_loss:.4f} | Val Acc: {val_acc:.4f} | LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
        "\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save({\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'epoch': epoch,\n",
        "                'val_acc': val_acc,\n",
        "                'label_map': train_ds.label_map\n",
        "            }, \"best_multimodal_model.pt\")\n",
        "            print(f\"âœ” Saved best model (Acc: {val_acc:.4f})\")\n",
        "            no_improve = 0\n",
        "        else:\n",
        "            no_improve += 1\n",
        "            if no_improve >= patience:\n",
        "                print(f\"\\nâš ï¸ Early stopping after {epoch+1} epochs (no improvement for {patience} epochs)\")\n",
        "                break\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    print(f\"\\nðŸŽ‰ Training complete. Best Val Acc = {best_acc:.4f}\")\n",
        "    return model, train_ds.label_map"
      ],
      "metadata": {
        "id": "NDxq-DzayALF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "# =====================================\n",
        "def test_model(model_path, test_csv, audio_root, label_map, batch_size=8):\n",
        "    \"\"\"Evaluate model performance in test dataset\"\"\"\n",
        "    from sklearn.metrics import classification_report, confusion_matrix\n",
        "    import numpy as np\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ðŸ“Š Testing on Test Set\")\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "    tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
        "    feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "    test_ds = PodcastDataset(test_csv, audio_root, tokenizer, feature_extractor)\n",
        "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    num_classes = len(label_map)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = ImprovedAudioTextClassifier(num_classes=num_classes, freeze_pretrained=False).to(device)\n",
        "\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(f\"âœ” Loaded best model from epoch {checkpoint['epoch']+1} (Val Acc: {checkpoint['val_acc']:.4f})\\n\")\n",
        "\n",
        "    test_acc, all_preds, all_labels = evaluate(model, test_loader, device)\n",
        "\n",
        "    idx_to_label = {v: k for k, v in label_map.items()}\n",
        "    class_names = [idx_to_label[i] for i in range(num_classes)]\n",
        "\n",
        "    print(f\"{'='*50}\")\n",
        "    print(f\"ðŸŽ¯ Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
        "    print(f\"{'='*50}\\n\")\n",
        "\n",
        "    print(\"ðŸ“‹ Classification Report:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(classification_report(all_labels, all_preds, target_names=class_names, digits=4))\n",
        "\n",
        "    print(\"\\nðŸ“Š Confusion Matrix:\")\n",
        "    print(\"-\" * 50)\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    header = \"True\\\\Pred  \" + \"  \".join([f\"{name[:8]:>8}\" for name in class_names])\n",
        "    print(header)\n",
        "    print(\"-\" * len(header))\n",
        "    for i, row in enumerate(cm):\n",
        "        row_str = f\"{class_names[i][:10]:<10} \" + \"  \".join([f\"{val:>8}\" for val in row])\n",
        "        print(row_str)\n",
        "\n",
        "    print(\"\\nðŸ“ˆ Per-Class Accuracy:\")\n",
        "    print(\"-\" * 50)\n",
        "    class_correct = cm.diagonal()\n",
        "    class_total = cm.sum(axis=1)\n",
        "    for i, name in enumerate(class_names):\n",
        "        acc = class_correct[i] / class_total[i] if class_total[i] > 0 else 0\n",
        "        print(f\"{name:20s}: {acc:.4f} ({class_correct[i]:3d}/{class_total[i]:3d})\")\n",
        "\n",
        "    print(\"\\nðŸ”€ Most Confused Class Pairs:\")\n",
        "    print(\"-\" * 50)\n",
        "    confusion_pairs = []\n",
        "    for i in range(num_classes):\n",
        "        for j in range(num_classes):\n",
        "            if i != j and cm[i, j] > 0:\n",
        "                confusion_pairs.append((cm[i, j], class_names[i], class_names[j]))\n",
        "\n",
        "    confusion_pairs.sort(reverse=True)\n",
        "    for count, true_class, pred_class in confusion_pairs[:10]:\n",
        "        print(f\"{true_class:15s} â†’ {pred_class:15s}: {count:3d} times\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"âœ… Testing Complete!\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    return test_acc, all_preds, all_labels"
      ],
      "metadata": {
        "id": "n_Qk5sJVyA79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run training and testing\n",
        "if __name__ == \"__main__\":\n",
        "    datadir = \"/content/drive/My Drive/CS441/FP2/data/\"\n",
        "\n",
        "    # train\n",
        "    model, label_map = train_model(\n",
        "        train_csv=os.path.join(datadir, \"train_transcripts.csv\"),\n",
        "        val_csv=os.path.join(datadir, \"val_transcripts.csv\"),\n",
        "        audio_root=datadir,\n",
        "        batch_size=8,\n",
        "        lr=2e-5,\n",
        "        epochs=20,\n",
        "        warmup_epochs=2\n",
        "    )\n",
        "\n",
        "    # evaluate\n",
        "    test_acc, test_preds, test_labels = test_model(\n",
        "        model_path=\"best_multimodal_model.pt\",\n",
        "        test_csv=os.path.join(datadir, \"test_transcripts.csv\"),\n",
        "        audio_root=datadir,\n",
        "        label_map=label_map,\n",
        "        batch_size=8\n",
        "    )"
      ],
      "metadata": {
        "id": "6a6uH3IfyGDd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bae408e9-79e9-4e51-a4d4-44b3a8c8da37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“Š Loaded 1080 samples, 6 classes: ['business', 'comedy', 'education', 'news', 'religion', 'sports']\n",
            "ðŸ“Š Loaded 240 samples, 6 classes: ['business', 'comedy', 'education', 'news', 'religion', 'sports']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸš€ Starting training...\n",
            "Device: cuda\n",
            "Total parameters: 162,446,472\n",
            "Trainable parameters: 119,919,240\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135/135 [00:33<00:00,  4.00it/s, loss=1.6857]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Epoch 1 | Train Loss: 1.8040 | Val Acc: 0.3417 | LR: 2.00e-06\n",
            "âœ” Saved best model (Acc: 0.3417)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135/135 [00:32<00:00,  4.18it/s, loss=1.7868]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Epoch 2 | Train Loss: 1.6961 | Val Acc: 0.4958 | LR: 1.99e-06\n",
            "âœ” Saved best model (Acc: 0.4958)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135/135 [00:33<00:00,  4.08it/s, loss=1.5980]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Epoch 3 | Train Loss: 1.6163 | Val Acc: 0.5500 | LR: 1.95e-06\n",
            "âœ” Saved best model (Acc: 0.5500)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135/135 [00:31<00:00,  4.27it/s, loss=1.4534]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Epoch 4 | Train Loss: 1.4760 | Val Acc: 0.5708 | LR: 1.89e-06\n",
            "âœ” Saved best model (Acc: 0.5708)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135/135 [00:33<00:00,  4.07it/s, loss=1.3061]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Epoch 5 | Train Loss: 1.3538 | Val Acc: 0.5792 | LR: 1.81e-06\n",
            "âœ” Saved best model (Acc: 0.5792)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135/135 [00:33<00:00,  4.07it/s, loss=1.5471]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Epoch 6 | Train Loss: 1.2811 | Val Acc: 0.5917 | LR: 1.71e-06\n",
            "âœ” Saved best model (Acc: 0.5917)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135/135 [00:33<00:00,  4.07it/s, loss=1.1430]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Epoch 7 | Train Loss: 1.1831 | Val Acc: 0.5958 | LR: 1.59e-06\n",
            "âœ” Saved best model (Acc: 0.5958)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135/135 [00:32<00:00,  4.10it/s, loss=1.0811]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Epoch 8 | Train Loss: 1.1534 | Val Acc: 0.5833 | LR: 1.45e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135/135 [00:31<00:00,  4.31it/s, loss=0.7985]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Epoch 9 | Train Loss: 1.0732 | Val Acc: 0.5792 | LR: 1.31e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135/135 [00:31<00:00,  4.28it/s, loss=1.1725]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Epoch 10 | Train Loss: 1.0263 | Val Acc: 0.6083 | LR: 1.16e-06\n",
            "âœ” Saved best model (Acc: 0.6083)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135/135 [00:33<00:00,  4.06it/s, loss=0.7862]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Epoch 11 | Train Loss: 1.0094 | Val Acc: 0.5875 | LR: 1.00e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135/135 [00:31<00:00,  4.30it/s, loss=0.7405]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Epoch 12 | Train Loss: 0.9625 | Val Acc: 0.6000 | LR: 8.44e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135/135 [00:31<00:00,  4.29it/s, loss=0.8308]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Epoch 13 | Train Loss: 0.9410 | Val Acc: 0.5958 | LR: 6.91e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135/135 [00:31<00:00,  4.24it/s, loss=1.4744]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Epoch 14 | Train Loss: 0.9180 | Val Acc: 0.6125 | LR: 5.46e-07\n",
            "âœ” Saved best model (Acc: 0.6125)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135/135 [00:31<00:00,  4.30it/s, loss=1.1453]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Epoch 15 | Train Loss: 0.8972 | Val Acc: 0.6125 | LR: 4.12e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135/135 [00:31<00:00,  4.28it/s, loss=0.9010]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Epoch 16 | Train Loss: 0.9094 | Val Acc: 0.6208 | LR: 2.93e-07\n",
            "âœ” Saved best model (Acc: 0.6208)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135/135 [00:32<00:00,  4.18it/s, loss=0.8812]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Epoch 17 | Train Loss: 0.8770 | Val Acc: 0.5917 | LR: 1.91e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135/135 [00:31<00:00,  4.24it/s, loss=0.8030]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Epoch 18 | Train Loss: 0.8858 | Val Acc: 0.6083 | LR: 1.09e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135/135 [00:31<00:00,  4.23it/s, loss=0.3825]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Epoch 19 | Train Loss: 0.8592 | Val Acc: 0.6083 | LR: 4.89e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135/135 [00:31<00:00,  4.29it/s, loss=1.0091]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Epoch 20 | Train Loss: 0.8591 | Val Acc: 0.6042 | LR: 1.23e-08\n",
            "\n",
            "ðŸŽ‰ Training complete. Best Val Acc = 0.6208\n",
            "\n",
            "==================================================\n",
            "ðŸ“Š Testing on Test Set\n",
            "==================================================\n",
            "\n",
            "ðŸ“Š Loaded 240 samples, 6 classes: ['business', 'comedy', 'education', 'news', 'religion', 'sports']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ” Loaded best model from epoch 16 (Val Acc: 0.6208)\n",
            "\n",
            "==================================================\n",
            "ðŸŽ¯ Test Accuracy: 0.7750 (77.50%)\n",
            "==================================================\n",
            "\n",
            "ðŸ“‹ Classification Report:\n",
            "--------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    business     0.6415    0.8500    0.7312        40\n",
            "      comedy     0.7297    0.6750    0.7013        40\n",
            "   education     0.6667    0.8500    0.7473        40\n",
            "        news     0.8571    0.4500    0.5902        40\n",
            "    religion     0.9756    1.0000    0.9877        40\n",
            "      sports     0.8919    0.8250    0.8571        40\n",
            "\n",
            "    accuracy                         0.7750       240\n",
            "   macro avg     0.7938    0.7750    0.7691       240\n",
            "weighted avg     0.7938    0.7750    0.7691       240\n",
            "\n",
            "\n",
            "ðŸ“Š Confusion Matrix:\n",
            "--------------------------------------------------\n",
            "True\\Pred  business    comedy  educatio      news  religion    sports\n",
            "---------------------------------------------------------------------\n",
            "business         34         1         4         1         0         0\n",
            "comedy            2        27         6         2         0         3\n",
            "education         5         0        34         0         0         1\n",
            "news             10         8         3        18         1         0\n",
            "religion          0         0         0         0        40         0\n",
            "sports            2         1         4         0         0        33\n",
            "\n",
            "ðŸ“ˆ Per-Class Accuracy:\n",
            "--------------------------------------------------\n",
            "business            : 0.8500 ( 34/ 40)\n",
            "comedy              : 0.6750 ( 27/ 40)\n",
            "education           : 0.8500 ( 34/ 40)\n",
            "news                : 0.4500 ( 18/ 40)\n",
            "religion            : 1.0000 ( 40/ 40)\n",
            "sports              : 0.8250 ( 33/ 40)\n",
            "\n",
            "ðŸ”€ Most Confused Class Pairs:\n",
            "--------------------------------------------------\n",
            "news            â†’ business       :  10 times\n",
            "news            â†’ comedy         :   8 times\n",
            "comedy          â†’ education      :   6 times\n",
            "education       â†’ business       :   5 times\n",
            "sports          â†’ education      :   4 times\n",
            "business        â†’ education      :   4 times\n",
            "news            â†’ education      :   3 times\n",
            "comedy          â†’ sports         :   3 times\n",
            "sports          â†’ business       :   2 times\n",
            "comedy          â†’ news           :   2 times\n",
            "\n",
            "==================================================\n",
            "âœ… Testing Complete!\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tested"
      ],
      "metadata": {
        "id": "9Q-XcezhyjOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Accuracy: 0.7875 (78.75%)\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchaudio\n",
        "from transformers import (\n",
        "    DistilBertTokenizerFast,\n",
        "    DistilBertModel,\n",
        "    Wav2Vec2FeatureExtractor,\n",
        "    Wav2Vec2Model\n",
        ")\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from tqdm import tqdm\n",
        "\n",
        "# =====================================\n",
        "# Dataset (æ”¹è¿›ç‰ˆ)\n",
        "# =====================================\n",
        "class PodcastDataset(Dataset):\n",
        "    def __init__(self, csv_path, audio_root, tokenizer, feature_extractor, max_audio_len=5.0):\n",
        "        self.data = pd.read_csv(csv_path)\n",
        "        self.audio_root = audio_root\n",
        "        self.tokenizer = tokenizer\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.max_audio_len = max_audio_len  # æœ€å¤§éŸ³é¢‘é•¿åº¦ï¼ˆç§’ï¼‰\n",
        "\n",
        "        genres = sorted(self.data[\"genre\"].unique())\n",
        "        self.label_map = {g: i for i, g in enumerate(genres)}\n",
        "        print(f\"ðŸ“Š Loaded {len(self.data)} samples, {len(genres)} classes: {genres}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "\n",
        "        # ----- Audio -----\n",
        "        audio_path = os.path.join(self.audio_root, \"snippets\", row[\"genre\"], row[\"path\"])\n",
        "        if not os.path.exists(audio_path):\n",
        "            raise FileNotFoundError(f\"Audio not found: {audio_path}\")\n",
        "\n",
        "        wav, sr = torchaudio.load(audio_path)\n",
        "\n",
        "        # è½¬ä¸ºå•å£°é“\n",
        "        if wav.size(0) > 1:\n",
        "            wav = wav.mean(dim=0, keepdim=True)\n",
        "\n",
        "        # é‡é‡‡æ ·åˆ°16kHz\n",
        "        if sr != 16000:\n",
        "            resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n",
        "            wav = resampler(wav)\n",
        "            sr = 16000\n",
        "\n",
        "        # è£å‰ªæˆ–å¡«å……åˆ°å›ºå®šé•¿åº¦\n",
        "        target_len = int(self.max_audio_len * sr)\n",
        "        if wav.size(1) > target_len:\n",
        "            wav = wav[:, :target_len]\n",
        "        elif wav.size(1) < target_len:\n",
        "            pad_len = target_len - wav.size(1)\n",
        "            wav = torch.nn.functional.pad(wav, (0, pad_len))\n",
        "\n",
        "        # å½’ä¸€åŒ–éŸ³é¢‘\n",
        "        wav = wav / (wav.abs().max() + 1e-8)\n",
        "\n",
        "        audio_features = self.feature_extractor(\n",
        "            wav.squeeze().numpy(), sampling_rate=sr, return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # ----- Text -----\n",
        "        text = row[\"transcript\"]\n",
        "        max_len = 256\n",
        "\n",
        "        if pd.isna(text) or text.strip() == \"\":\n",
        "            use_text = False\n",
        "            input_ids = torch.zeros(max_len, dtype=torch.long)\n",
        "            attention_mask = torch.zeros(max_len, dtype=torch.long)\n",
        "        else:\n",
        "            use_text = True\n",
        "            encoded_text = self.tokenizer(\n",
        "                text,\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "                max_length=max_len,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "            input_ids = encoded_text[\"input_ids\"].squeeze(0)\n",
        "            attention_mask = encoded_text[\"attention_mask\"].squeeze(0)\n",
        "\n",
        "        label = self.label_map[row[\"genre\"]]\n",
        "\n",
        "        return {\n",
        "            \"audio_values\": audio_features[\"input_values\"].squeeze(0),\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": torch.tensor(label),\n",
        "            \"use_text\": torch.tensor(use_text, dtype=torch.bool)\n",
        "        }\n",
        "\n",
        "\n",
        "# =====================================\n",
        "# æ”¹è¿›çš„æ¨¡åž‹æž¶æž„\n",
        "# =====================================\n",
        "class ImprovedAudioTextClassifier(nn.Module):\n",
        "    def __init__(self, num_classes, freeze_pretrained=False):  # æ”¹ä¸ºä¸å†»ç»“\n",
        "        super().__init__()\n",
        "\n",
        "        # Audio encoder\n",
        "        self.audio_encoder = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "        # Text encoder\n",
        "        self.text_encoder = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "        if freeze_pretrained:\n",
        "            for param in self.audio_encoder.parameters():\n",
        "                param.requires_grad = False\n",
        "            for param in self.text_encoder.parameters():\n",
        "                param.requires_grad = False\n",
        "        else:\n",
        "            # åªå¾®è°ƒæœ€åŽå‡ å±‚\n",
        "            # å†»ç»“ Wav2Vec2 çš„å‰ 8 å±‚\n",
        "            for i, layer in enumerate(self.audio_encoder.encoder.layers):\n",
        "                if i < 4:\n",
        "                    for param in layer.parameters():\n",
        "                        param.requires_grad = False\n",
        "\n",
        "            # å†»ç»“ DistilBERT çš„å‰ 4 å±‚\n",
        "            for i, layer in enumerate(self.text_encoder.transformer.layer):\n",
        "                if i < 2:\n",
        "                    for param in layer.parameters():\n",
        "                        param.requires_grad = False\n",
        "\n",
        "        # æ”¹è¿›çš„æŠ•å½±å±‚ï¼ˆå¸¦BatchNormå’ŒDropoutï¼‰\n",
        "        self.audio_proj = nn.Sequential(\n",
        "            nn.Linear(768, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "\n",
        "        self.text_proj = nn.Sequential(\n",
        "            nn.Linear(768, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "\n",
        "        # æ³¨æ„åŠ›èžåˆå±‚ï¼ˆæ›¿ä»£ç®€å•æ‹¼æŽ¥ï¼‰\n",
        "        self.fusion_attention = nn.Sequential(\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(512, 2),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "        # æ”¹è¿›çš„åˆ†ç±»å™¨\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_audio, input_ids=None, attention_mask=None, use_text=None):\n",
        "        # ===== éŸ³é¢‘ç‰¹å¾ =====\n",
        "        audio_out = self.audio_encoder(input_audio).last_hidden_state\n",
        "        audio_feat = audio_out.mean(dim=1)  # [batch_size, 768]\n",
        "        audio_feat = self.audio_proj(audio_feat)  # [batch_size, 512]\n",
        "\n",
        "        # ===== æ–‡æœ¬ç‰¹å¾ =====\n",
        "        batch_size = audio_feat.size(0)\n",
        "        text_feat = torch.zeros(batch_size, 512, device=audio_feat.device, dtype=audio_feat.dtype)\n",
        "\n",
        "        if use_text is not None:\n",
        "            text_mask = use_text.bool()\n",
        "\n",
        "            if text_mask.any():\n",
        "                valid_input_ids = input_ids[text_mask]\n",
        "                valid_attention_mask = attention_mask[text_mask]\n",
        "\n",
        "                text_out = self.text_encoder(\n",
        "                    input_ids=valid_input_ids,\n",
        "                    attention_mask=valid_attention_mask\n",
        "                ).last_hidden_state\n",
        "\n",
        "                text_feat_valid = text_out.mean(dim=1)\n",
        "                text_feat_valid = self.text_proj(text_feat_valid)\n",
        "                text_feat[text_mask] = text_feat_valid\n",
        "\n",
        "        # ===== æ³¨æ„åŠ›èžåˆï¼ˆæ›¿ä»£ç®€å•æ‹¼æŽ¥ï¼‰=====\n",
        "        fused = torch.cat([audio_feat, text_feat], dim=1)  # [batch_size, 1024]\n",
        "\n",
        "        # è®¡ç®—æ³¨æ„åŠ›æƒé‡\n",
        "        attn_weights = self.fusion_attention(fused)  # [batch_size, 2]\n",
        "\n",
        "        # åŠ æƒèžåˆ\n",
        "        weighted_audio = audio_feat * attn_weights[:, 0:1]\n",
        "        weighted_text = text_feat * attn_weights[:, 1:2]\n",
        "        final_feat = weighted_audio + weighted_text  # [batch_size, 512]\n",
        "\n",
        "        logits = self.classifier(final_feat)\n",
        "        return logits\n",
        "\n",
        "\n",
        "# =====================================\n",
        "# Train / Evalï¼ˆæ”¹è¿›ç‰ˆï¼‰\n",
        "# =====================================\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            logits = model(\n",
        "                input_audio=batch[\"audio_values\"].to(device),\n",
        "                input_ids=batch[\"input_ids\"].to(device),\n",
        "                attention_mask=batch[\"attention_mask\"].to(device),\n",
        "                use_text=batch[\"use_text\"].to(device)\n",
        "            )\n",
        "            preds = logits.argmax(dim=1)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy, all_preds, all_labels\n",
        "\n",
        "\n",
        "def train_model(train_csv, val_csv, audio_root, batch_size=8, lr=2e-5, epochs=10, warmup_epochs=2):\n",
        "    tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
        "    feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "    train_ds = PodcastDataset(train_csv, audio_root, tokenizer, feature_extractor)\n",
        "    val_ds = PodcastDataset(val_csv, audio_root, tokenizer, feature_extractor)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    num_classes = len(train_ds.label_map)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # ä½¿ç”¨æ”¹è¿›çš„æ¨¡åž‹\n",
        "    model = ImprovedAudioTextClassifier(num_classes=num_classes, freeze_pretrained=False).to(device)\n",
        "\n",
        "    # åˆ†ç»„å­¦ä¹ çŽ‡\n",
        "    pretrained_params = []\n",
        "    new_params = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'audio_encoder' in name or 'text_encoder' in name:\n",
        "            pretrained_params.append(param)\n",
        "        else:\n",
        "            new_params.append(param)\n",
        "\n",
        "    optimizer = AdamW([\n",
        "        {'params': pretrained_params, 'lr': lr * 0.1},  # é¢„è®­ç»ƒå±‚ç”¨æ›´å°å­¦ä¹ çŽ‡\n",
        "        {'params': new_params, 'lr': lr}\n",
        "    ])\n",
        "\n",
        "    # æ·»åŠ å­¦ä¹ çŽ‡è°ƒåº¦å™¨\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    best_acc = 0\n",
        "    patience = 5\n",
        "    no_improve = 0\n",
        "\n",
        "    print(f\"\\nðŸš€ Starting training...\")\n",
        "    print(f\"Device: {device}\")\n",
        "    print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "    print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\\n\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "        for batch in pbar:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            logits = model(\n",
        "                input_audio=batch[\"audio_values\"].to(device),\n",
        "                input_ids=batch[\"input_ids\"].to(device),\n",
        "                attention_mask=batch[\"attention_mask\"].to(device),\n",
        "                use_text=batch[\"use_text\"].to(device)\n",
        "            )\n",
        "            loss = criterion(logits, batch[\"labels\"].to(device))\n",
        "\n",
        "            if torch.isnan(loss) or torch.isinf(loss):\n",
        "                print(f\"\\nâš ï¸ Warning: Invalid loss detected, skipping batch\")\n",
        "                continue\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        val_acc, _, _ = evaluate(model, val_loader, device)\n",
        "\n",
        "        print(f\">>> Epoch {epoch+1} | Train Loss: {avg_loss:.4f} | Val Acc: {val_acc:.4f} | LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
        "\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save({\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'epoch': epoch,\n",
        "                'val_acc': val_acc,\n",
        "                'label_map': train_ds.label_map\n",
        "            }, \"best_multimodal_model.pt\")\n",
        "            print(f\"âœ” Saved best model (Acc: {val_acc:.4f})\")\n",
        "            no_improve = 0\n",
        "        else:\n",
        "            no_improve += 1\n",
        "            if no_improve >= patience:\n",
        "                print(f\"\\nâš ï¸ Early stopping after {epoch+1} epochs (no improvement for {patience} epochs)\")\n",
        "                break\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    print(f\"\\nðŸŽ‰ Training complete. Best Val Acc = {best_acc:.4f}\")\n",
        "    return model, train_ds.label_map\n",
        "\n",
        "\n",
        "# =====================================\n",
        "# Testå‡½æ•°\n",
        "# =====================================\n",
        "def test_model(model_path, test_csv, audio_root, label_map, batch_size=8):\n",
        "    \"\"\"åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡åž‹æ€§èƒ½\"\"\"\n",
        "    from sklearn.metrics import classification_report, confusion_matrix\n",
        "    import numpy as np\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ðŸ“Š Testing on Test Set\")\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "    # åŠ è½½æ¨¡åž‹\n",
        "    tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
        "    feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "    test_ds = PodcastDataset(test_csv, audio_root, tokenizer, feature_extractor)\n",
        "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    num_classes = len(label_map)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = ImprovedAudioTextClassifier(num_classes=num_classes, freeze_pretrained=False).to(device)\n",
        "\n",
        "    # åŠ è½½æœ€ä½³æ¨¡åž‹æƒé‡\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(f\"âœ” Loaded best model from epoch {checkpoint['epoch']+1} (Val Acc: {checkpoint['val_acc']:.4f})\\n\")\n",
        "\n",
        "    # è¯„ä¼°\n",
        "    test_acc, all_preds, all_labels = evaluate(model, test_loader, device)\n",
        "\n",
        "    # åè½¬ label_map ä»¥èŽ·å–ç±»åˆ«åç§°\n",
        "    idx_to_label = {v: k for k, v in label_map.items()}\n",
        "    class_names = [idx_to_label[i] for i in range(num_classes)]\n",
        "\n",
        "    # æ‰“å°æ€»ä½“ç»“æžœ\n",
        "    print(f\"{'='*50}\")\n",
        "    print(f\"ðŸŽ¯ Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
        "    print(f\"{'='*50}\\n\")\n",
        "\n",
        "    # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š\n",
        "    print(\"ðŸ“‹ Classification Report:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(classification_report(all_labels, all_preds, target_names=class_names, digits=4))\n",
        "\n",
        "    # æ··æ·†çŸ©é˜µ\n",
        "    print(\"\\nðŸ“Š Confusion Matrix:\")\n",
        "    print(\"-\" * 50)\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    # æ‰“å°æ ¼å¼åŒ–çš„æ··æ·†çŸ©é˜µ\n",
        "    header = \"True\\\\Pred  \" + \"  \".join([f\"{name[:8]:>8}\" for name in class_names])\n",
        "    print(header)\n",
        "    print(\"-\" * len(header))\n",
        "    for i, row in enumerate(cm):\n",
        "        row_str = f\"{class_names[i][:10]:<10} \" + \"  \".join([f\"{val:>8}\" for val in row])\n",
        "        print(row_str)\n",
        "\n",
        "    # æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®çŽ‡\n",
        "    print(\"\\nðŸ“ˆ Per-Class Accuracy:\")\n",
        "    print(\"-\" * 50)\n",
        "    class_correct = cm.diagonal()\n",
        "    class_total = cm.sum(axis=1)\n",
        "    for i, name in enumerate(class_names):\n",
        "        acc = class_correct[i] / class_total[i] if class_total[i] > 0 else 0\n",
        "        print(f\"{name:20s}: {acc:.4f} ({class_correct[i]:3d}/{class_total[i]:3d})\")\n",
        "\n",
        "    # æœ€å®¹æ˜“æ··æ·†çš„ç±»åˆ«å¯¹\n",
        "    print(\"\\nðŸ”€ Most Confused Class Pairs:\")\n",
        "    print(\"-\" * 50)\n",
        "    confusion_pairs = []\n",
        "    for i in range(num_classes):\n",
        "        for j in range(num_classes):\n",
        "            if i != j and cm[i, j] > 0:\n",
        "                confusion_pairs.append((cm[i, j], class_names[i], class_names[j]))\n",
        "\n",
        "    confusion_pairs.sort(reverse=True)\n",
        "    for count, true_class, pred_class in confusion_pairs[:10]:\n",
        "        print(f\"{true_class:15s} â†’ {pred_class:15s}: {count:3d} times\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"âœ… Testing Complete!\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    return test_acc, all_preds, all_labels\n",
        "\n",
        "\n",
        "# =====================================\n",
        "# Run training and testing\n",
        "# =====================================\n",
        "if __name__ == \"__main__\":\n",
        "    datadir = \"/content/drive/My Drive/CS441/FP2/data/\"\n",
        "\n",
        "    # è®­ç»ƒæ¨¡åž‹\n",
        "    model, label_map = train_model(\n",
        "        train_csv=os.path.join(datadir, \"train_transcripts.csv\"),\n",
        "        val_csv=os.path.join(datadir, \"val_transcripts.csv\"),\n",
        "        audio_root=datadir,\n",
        "        batch_size=8,\n",
        "        lr=2e-5,\n",
        "        epochs=20,\n",
        "        warmup_epochs=2\n",
        "    )\n",
        "\n",
        "    # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°\n",
        "    test_acc, test_preds, test_labels = test_model(\n",
        "        model_path=\"best_multimodal_model.pt\",\n",
        "        test_csv=os.path.join(datadir, \"test_transcripts.csv\"),\n",
        "        audio_root=datadir,\n",
        "        label_map=label_map,\n",
        "        batch_size=8\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xA8riHjghjA",
        "outputId": "93bc2697-4d18-4c5c-bc54-639446bd8297"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“Š Loaded 1080 samples, 6 classes: ['business', 'comedy', 'education', 'news', 'religion', 'sports']\n",
            "ðŸ“Š Loaded 240 samples, 6 classes: ['business', 'comedy', 'education', 'news', 'religion', 'sports']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸš€ Starting training...\n",
            "Device: cuda\n",
            "Total parameters: 162,446,472\n",
            "Trainable parameters: 119,919,240\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135/135 [00:34<00:00,  3.96it/s, loss=1.7530]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Epoch 1 | Train Loss: 1.8151 | Val Acc: 0.3125 | LR: 2.00e-06\n",
            "âœ” Saved best model (Acc: 0.3125)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135/135 [00:33<00:00,  4.01it/s, loss=1.6621]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Epoch 2 | Train Loss: 1.7149 | Val Acc: 0.5083 | LR: 1.99e-06\n",
            "âœ” Saved best model (Acc: 0.5083)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135/135 [00:34<00:00,  3.93it/s, loss=1.3811]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Epoch 3 | Train Loss: 1.6056 | Val Acc: 0.5583 | LR: 1.95e-06\n",
            "âœ” Saved best model (Acc: 0.5583)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135/135 [00:32<00:00,  4.11it/s, loss=1.5300]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Epoch 4 | Train Loss: 1.4858 | Val Acc: 0.5542 | LR: 1.89e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135/135 [00:32<00:00,  4.13it/s, loss=1.4863]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Epoch 5 | Train Loss: 1.3547 | Val Acc: 0.5500 | LR: 1.81e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135/135 [00:32<00:00,  4.12it/s, loss=0.9473]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Epoch 6 | Train Loss: 1.2586 | Val Acc: 0.5417 | LR: 1.71e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135/135 [00:32<00:00,  4.16it/s, loss=1.1882]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Epoch 7 | Train Loss: 1.1893 | Val Acc: 0.5417 | LR: 1.59e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135/135 [00:32<00:00,  4.16it/s, loss=0.9935]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Epoch 8 | Train Loss: 1.1412 | Val Acc: 0.5708 | LR: 1.45e-06\n",
            "âœ” Saved best model (Acc: 0.5708)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135/135 [00:34<00:00,  3.96it/s, loss=1.3785]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Epoch 9 | Train Loss: 1.0776 | Val Acc: 0.5792 | LR: 1.31e-06\n",
            "âœ” Saved best model (Acc: 0.5792)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135/135 [00:34<00:00,  3.90it/s, loss=1.3659]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Epoch 10 | Train Loss: 1.0407 | Val Acc: 0.6000 | LR: 1.16e-06\n",
            "âœ” Saved best model (Acc: 0.6000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135/135 [00:33<00:00,  4.06it/s, loss=1.0686]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Epoch 11 | Train Loss: 1.0012 | Val Acc: 0.5958 | LR: 1.00e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135/135 [00:32<00:00,  4.16it/s, loss=0.7608]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Epoch 12 | Train Loss: 0.9661 | Val Acc: 0.5792 | LR: 8.44e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135/135 [00:32<00:00,  4.15it/s, loss=1.1121]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Epoch 13 | Train Loss: 0.9539 | Val Acc: 0.6000 | LR: 6.91e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135/135 [00:32<00:00,  4.17it/s, loss=1.0329]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Epoch 14 | Train Loss: 0.8933 | Val Acc: 0.5917 | LR: 5.46e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135/135 [00:32<00:00,  4.18it/s, loss=1.1337]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Epoch 15 | Train Loss: 0.9118 | Val Acc: 0.6000 | LR: 4.12e-07\n",
            "\n",
            "âš ï¸ Early stopping after 15 epochs (no improvement for 5 epochs)\n",
            "\n",
            "ðŸŽ‰ Training complete. Best Val Acc = 0.6000\n",
            "\n",
            "==================================================\n",
            "ðŸ“Š Testing on Test Set\n",
            "==================================================\n",
            "\n",
            "ðŸ“Š Loaded 240 samples, 6 classes: ['business', 'comedy', 'education', 'news', 'religion', 'sports']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ” Loaded best model from epoch 10 (Val Acc: 0.6000)\n",
            "\n",
            "==================================================\n",
            "ðŸŽ¯ Test Accuracy: 0.7875 (78.75%)\n",
            "==================================================\n",
            "\n",
            "ðŸ“‹ Classification Report:\n",
            "--------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    business     0.6981    0.9250    0.7957        40\n",
            "      comedy     0.7105    0.6750    0.6923        40\n",
            "   education     0.8000    0.7000    0.7467        40\n",
            "        news     0.7097    0.5500    0.6197        40\n",
            "    religion     0.9524    1.0000    0.9756        40\n",
            "      sports     0.8537    0.8750    0.8642        40\n",
            "\n",
            "    accuracy                         0.7875       240\n",
            "   macro avg     0.7874    0.7875    0.7824       240\n",
            "weighted avg     0.7874    0.7875    0.7824       240\n",
            "\n",
            "\n",
            "ðŸ“Š Confusion Matrix:\n",
            "--------------------------------------------------\n",
            "True\\Pred  business    comedy  educatio      news  religion    sports\n",
            "---------------------------------------------------------------------\n",
            "business         37         0         2         1         0         0\n",
            "comedy            4        27         2         3         0         4\n",
            "education         4         2        28         3         1         2\n",
            "news              7         8         2        22         1         0\n",
            "religion          0         0         0         0        40         0\n",
            "sports            1         1         1         2         0        35\n",
            "\n",
            "ðŸ“ˆ Per-Class Accuracy:\n",
            "--------------------------------------------------\n",
            "business            : 0.9250 ( 37/ 40)\n",
            "comedy              : 0.6750 ( 27/ 40)\n",
            "education           : 0.7000 ( 28/ 40)\n",
            "news                : 0.5500 ( 22/ 40)\n",
            "religion            : 1.0000 ( 40/ 40)\n",
            "sports              : 0.8750 ( 35/ 40)\n",
            "\n",
            "ðŸ”€ Most Confused Class Pairs:\n",
            "--------------------------------------------------\n",
            "news            â†’ comedy         :   8 times\n",
            "news            â†’ business       :   7 times\n",
            "education       â†’ business       :   4 times\n",
            "comedy          â†’ sports         :   4 times\n",
            "comedy          â†’ business       :   4 times\n",
            "education       â†’ news           :   3 times\n",
            "comedy          â†’ news           :   3 times\n",
            "sports          â†’ news           :   2 times\n",
            "news            â†’ education      :   2 times\n",
            "education       â†’ sports         :   2 times\n",
            "\n",
            "==================================================\n",
            "âœ… Testing Complete!\n",
            "==================================================\n"
          ]
        }
      ]
    }
  ]
}